Description: >
  This project is script is for creating Workers  nodes
  
Parameters:
    
    EnvironmentName:
            Description: An environment name that will be prefixed to resource names
            Type: String
        
    ClusterName:
            Description: The name of the cluster.
            Type: String
            
    OnDemandBootstrapArguments:
            Description: Sets Node Labels to set lifecycle as OnDemand
            Type: String

    OnDemandNodeGroupName:
            Description: Unique identifier for the OnDemand Node Group.
            Type: String
            Default: "OnDemandNodeGroup"

Resources:

  NodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: "/"
      Roles:
        - !Ref NodeInstanceRole

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      Policies:
        - PolicyName: ClusterAutoscaler
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Sid: K8NodeASGPerms
                Effect: Allow
                Action:
                  - autoscaling:DescribeAutoScalingGroups
                  - autoscaling:DescribeAutoScalingInstances
                  - autoscaling:DescribeLaunchConfigurations
                  - autoscaling:SetDesiredCapacity
                  - autoscaling:DescribeTags
                  - autoscaling:TerminateInstanceInAutoScalingGroup
                  - autoscaling:DescribeTags
                Resource: "*"

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for all nodes in the cluster
      VpcId:
        Fn::ImportValue:
          !Sub "${EnvironmentName}-VPCID"
      Tags:
        - Key: !Sub "kubernetes.io/cluster/${ClusterName}"
          Value: "owned"

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow node to communicate with each other
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: "-1"
      FromPort: 0
      ToPort: 65535

  NodeSecurityGroupFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow worker Kubelets and pods to receive communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId:
        Fn::ImportValue:
            !Sub ${EnvironmentName}-SecurityGroups
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  AllowHTTPSFromControlPlaneIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow worker Kubelets and pods to receive HTTP communication from the cluster control plane
      GroupId: !Ref NodeSecurityGroup
      SourceSecurityGroupId:
        Fn::ImportValue:
            !Sub ${EnvironmentName}-SecurityGroups
      IpProtocol: tcp
      FromPort: 80
      ToPort: 80

  ControlPlaneEgressToNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroupEgress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow the cluster control plane to communicate with worker Kubelet and pods
      GroupId: 
        Fn::ImportValue:
            !Sub ${EnvironmentName}-SecurityGroups
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 1025
      ToPort: 65535

  ControlPlaneEgressToNodeSecurityGroupOn443:
    Type: AWS::EC2::SecurityGroupEgress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow the cluster control plane to communicate with pods running extension API servers on port 80
      GroupId: 
        Fn::ImportValue:
            !Sub ${EnvironmentName}-SecurityGroups
      DestinationSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      FromPort: 80
      ToPort: 80

  ClusterControlPlaneSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: NodeSecurityGroup
    Properties:
      Description: Allow pods to communicate with the cluster API Server
      GroupId:
        Fn::ImportValue:
            !Sub ${EnvironmentName}-SecurityGroups
      SourceSecurityGroupId: !Ref NodeSecurityGroup
      IpProtocol: tcp
      ToPort: 80
      FromPort: 80

  OnDemandNodeGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: 1
      LaunchConfigurationName: !Ref OnDemandNodeLaunchConfig
      MinSize: 1
      MaxSize: 2
      VPCZoneIdentifier:
      - Fn::ImportValue: 
          !Sub "${EnvironmentName}-PUB-NETS"
      Tags:
        - Key: Name
          Value: !Sub "${ClusterName}-${OnDemandNodeGroupName}-Node"
          PropagateAtLaunch: "true"
        - Key: !Sub "kubernetes.io/cluster/${ClusterName}"
          Value: "owned"
          PropagateAtLaunch: "true"
        - Key: Spot
          Value: "false"
          PropagateAtLaunch: "true"
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: "1"
        MaxBatchSize: "1"

  OnDemandNodeLaunchConfig:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: "true"
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: "ami-0ef76ba092ce4e253"
      InstanceType: "t2.medium"
      KeyName: "pipeline"
      SecurityGroups:
        - !Ref NodeSecurityGroup
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 20
            VolumeType: "gp2"
            DeleteOnTermination: true
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -o xtrace
          /etc/eks/bootstrap.sh ${ClusterName} ${OnDemandBootstrapArguments}
          /opt/aws/bin/cfn-signal --exit-code $? \
                   --stack  ${AWS::StackName} \
                   --resource OnDemandNodeGroup  \
                   --region ${AWS::Region}